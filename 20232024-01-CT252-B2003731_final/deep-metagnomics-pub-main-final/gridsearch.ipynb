{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4a0893c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 972 candidates, totalling 4860 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuandai/.local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters (Tuning): {'max_depth': 30, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100, 'random_state': None}\n",
      "Test Accuracy (Final Model): 0.9515885022692889\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load your data\n",
    "data = pd.read_csv('/mnt/d/20232024-01-CT252-B2003731_final/20232024-01-CT252-B2003731_final/deep-metagnomics-pub-main-final/data/t2d_x.csv')\n",
    "\n",
    "data.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "# Transpose the data\n",
    "data_transposed = data.transpose()\n",
    "\n",
    "# Assuming the last column contains labels\n",
    "label_column = data_transposed.columns[-1]\n",
    "\n",
    "# Define features and labels\n",
    "X = data_transposed.drop(columns=[label_column])\n",
    "y = data_transposed[label_column]\n",
    "\n",
    "# Convert labels to strings\n",
    "y = y.astype(str)\n",
    "\n",
    "# Apply LabelEncoder to labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Chia dữ liệu thành tập huấn luyện và tập kiểm tra\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Thiết lập các siêu tham số để thử nghiệm\n",
    "param_grid = {\n",
    "    \n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'random_state': [None, 42, 100],  # Đặt giá trị random_state tùy ý\n",
    "    'max_features': ['auto', 'sqrt', 'log2']  # Các cách chọn số features khi tìm split node\n",
    "\n",
    "}\n",
    "\n",
    "# Tạo mô hình RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "# Chia dữ liệu thành tập huấn luyện chính và phần dữ liệu nhỏ để tinh chỉnh tham số\n",
    "X_train_main, X_train_tune, y_train_main, y_train_tune = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Thiết lập grid search trên phần dữ liệu nhỏ để tinh chỉnh tham số\n",
    "grid_search_tune = GridSearchCV(rf, param_grid, cv=5, verbose=1)\n",
    "grid_search_tune.fit(X_train_tune, y_train_tune)\n",
    "\n",
    "# Siêu tham số tốt nhất từ phần dữ liệu nhỏ\n",
    "best_params_tune = grid_search_tune.best_params_\n",
    "print(\"Best Parameters (Tuning):\", best_params_tune)\n",
    "\n",
    "# Sử dụng toàn bộ dữ liệu huấn luyện chính để fit mô hình với siêu tham số tốt nhất từ phần tinh chỉnh\n",
    "best_model_main = RandomForestClassifier(**best_params_tune)\n",
    "best_model_main.fit(X_train_main, y_train_main)\n",
    "\n",
    "# Đánh giá mô hình trên tập kiểm tra\n",
    "test_score_main = best_model_main.score(X_test, y_test)\n",
    "print(\"Test Accuracy (Final Model):\", test_score_main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfcbe0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 972 candidates, totalling 4860 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuandai/.local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters (Tuning): {'max_depth': None, 'max_features': 'auto', 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 200, 'random_state': 42}\n",
      "Test Accuracy (Final Model): 0.9258698940998488\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load your data\n",
    "data = pd.read_csv('/mnt/d/20232024-01-CT252-B2003731_final/20232024-01-CT252-B2003731_final/deep-metagnomics-pub-main-final/data/WT2D_x.csv')\n",
    "\n",
    "data.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "# Transpose the data\n",
    "data_transposed = data.transpose()\n",
    "\n",
    "# Assuming the last column contains labels\n",
    "label_column = data_transposed.columns[-1]\n",
    "\n",
    "# Define features and labels\n",
    "X = data_transposed.drop(columns=[label_column])\n",
    "y = data_transposed[label_column]\n",
    "\n",
    "# Convert labels to strings\n",
    "y = y.astype(str)\n",
    "\n",
    "# Apply LabelEncoder to labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Chia dữ liệu thành tập huấn luyện và tập kiểm tra\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Thiết lập các siêu tham số để thử nghiệm\n",
    "param_grid = {\n",
    "    \n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'random_state': [None, 42, 100],  # Đặt giá trị random_state tùy ý\n",
    "    'max_features': ['auto', 'sqrt', 'log2']  # Các cách chọn số features khi tìm split node\n",
    "\n",
    "}\n",
    "\n",
    "# Tạo mô hình RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "# Chia dữ liệu thành tập huấn luyện chính và phần dữ liệu nhỏ để tinh chỉnh tham số\n",
    "X_train_main, X_train_tune, y_train_main, y_train_tune = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Thiết lập grid search trên phần dữ liệu nhỏ để tinh chỉnh tham số\n",
    "grid_search_tune = GridSearchCV(rf, param_grid, cv=5, verbose=1)\n",
    "grid_search_tune.fit(X_train_tune, y_train_tune)\n",
    "\n",
    "# Siêu tham số tốt nhất từ phần dữ liệu nhỏ\n",
    "best_params_tune = grid_search_tune.best_params_\n",
    "print(\"Best Parameters (Tuning):\", best_params_tune)\n",
    "\n",
    "# Sử dụng toàn bộ dữ liệu huấn luyện chính để fit mô hình với siêu tham số tốt nhất từ phần tinh chỉnh\n",
    "best_model_main = RandomForestClassifier(**best_params_tune)\n",
    "best_model_main.fit(X_train_main, y_train_main)\n",
    "\n",
    "# Đánh giá mô hình trên tập kiểm tra\n",
    "test_score_main = best_model_main.score(X_test, y_test)\n",
    "print(\"Test Accuracy (Final Model):\", test_score_main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92b6359e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 972 candidates, totalling 4860 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuandai/.local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters (Tuning): {'max_depth': 30, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100, 'random_state': None}\n",
      "Test Accuracy (Final Model): 0.9515885022692889\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load your data\n",
    "data = pd.read_csv('/mnt/d/20232024-01-CT252-B2003731_final/20232024-01-CT252-B2003731_final/deep-metagnomics-pub-main-final/data/Zeller_fecal_colorectal_cancer_x.csv')\n",
    "\n",
    "data.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "# Transpose the data\n",
    "data_transposed = data.transpose()\n",
    "\n",
    "# Assuming the last column contains labels\n",
    "label_column = data_transposed.columns[-1]\n",
    "\n",
    "# Define features and labels\n",
    "X = data_transposed.drop(columns=[label_column])\n",
    "y = data_transposed[label_column]\n",
    "\n",
    "# Convert labels to strings\n",
    "y = y.astype(str)\n",
    "\n",
    "# Apply LabelEncoder to labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Chia dữ liệu thành tập huấn luyện và tập kiểm tra\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Thiết lập các siêu tham số để thử nghiệm\n",
    "param_grid = {\n",
    "    \n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'random_state': [None, 42, 100],  # Đặt giá trị random_state tùy ý\n",
    "    'max_features': ['auto', 'sqrt', 'log2']  # Các cách chọn số features khi tìm split node\n",
    "\n",
    "}\n",
    "\n",
    "# Tạo mô hình RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "# Chia dữ liệu thành tập huấn luyện chính và phần dữ liệu nhỏ để tinh chỉnh tham số\n",
    "X_train_main, X_train_tune, y_train_main, y_train_tune = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Thiết lập grid search trên phần dữ liệu nhỏ để tinh chỉnh tham số\n",
    "grid_search_tune = GridSearchCV(rf, param_grid, cv=5, verbose=1)\n",
    "grid_search_tune.fit(X_train_tune, y_train_tune)\n",
    "\n",
    "# Siêu tham số tốt nhất từ phần dữ liệu nhỏ\n",
    "best_params_tune = grid_search_tune.best_params_\n",
    "print(\"Best Parameters (Tuning):\", best_params_tune)\n",
    "\n",
    "# Sử dụng toàn bộ dữ liệu huấn luyện chính để fit mô hình với siêu tham số tốt nhất từ phần tinh chỉnh\n",
    "best_model_main = RandomForestClassifier(**best_params_tune)\n",
    "best_model_main.fit(X_train_main, y_train_main)\n",
    "\n",
    "# Đánh giá mô hình trên tập kiểm tra\n",
    "test_score_main = best_model_main.score(X_test, y_test)\n",
    "print(\"Test Accuracy (Final Model):\", test_score_main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52182760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 972 candidates, totalling 4860 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuandai/.local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters (Tuning): {'max_depth': None, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100, 'random_state': None}\n",
      "Test Accuracy (Final Model): 0.9546142208774584\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load your data\n",
    "data = pd.read_csv('/mnt/d/20232024-01-CT252-B2003731_final/20232024-01-CT252-B2003731_final/deep-metagnomics-pub-main-final/data/Quin_gut_liver_cirrhosis_x.csv')\n",
    "\n",
    "data.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "# Transpose the data\n",
    "data_transposed = data.transpose()\n",
    "\n",
    "# Assuming the last column contains labels\n",
    "label_column = data_transposed.columns[-1]\n",
    "\n",
    "# Define features and labels\n",
    "X = data_transposed.drop(columns=[label_column])\n",
    "y = data_transposed[label_column]\n",
    "\n",
    "# Convert labels to strings\n",
    "y = y.astype(str)\n",
    "\n",
    "# Apply LabelEncoder to labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Chia dữ liệu thành tập huấn luyện và tập kiểm tra\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Thiết lập các siêu tham số để thử nghiệm\n",
    "param_grid = {\n",
    "    \n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'random_state': [None, 42, 100],  # Đặt giá trị random_state tùy ý\n",
    "    'max_features': ['auto', 'sqrt', 'log2']  # Các cách chọn số features khi tìm split node\n",
    "\n",
    "}\n",
    "\n",
    "# Tạo mô hình RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "# Chia dữ liệu thành tập huấn luyện chính và phần dữ liệu nhỏ để tinh chỉnh tham số\n",
    "X_train_main, X_train_tune, y_train_main, y_train_tune = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Thiết lập grid search trên phần dữ liệu nhỏ để tinh chỉnh tham số\n",
    "grid_search_tune = GridSearchCV(rf, param_grid, cv=5, verbose=1)\n",
    "grid_search_tune.fit(X_train_tune, y_train_tune)\n",
    "\n",
    "# Siêu tham số tốt nhất từ phần dữ liệu nhỏ\n",
    "best_params_tune = grid_search_tune.best_params_\n",
    "print(\"Best Parameters (Tuning):\", best_params_tune)\n",
    "\n",
    "# Sử dụng toàn bộ dữ liệu huấn luyện chính để fit mô hình với siêu tham số tốt nhất từ phần tinh chỉnh\n",
    "best_model_main = RandomForestClassifier(**best_params_tune)\n",
    "best_model_main.fit(X_train_main, y_train_main)\n",
    "\n",
    "# Đánh giá mô hình trên tập kiểm tra\n",
    "test_score_main = best_model_main.score(X_test, y_test)\n",
    "print(\"Test Accuracy (Final Model):\", test_score_main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "339541da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-20 00:45:13.081388: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-20 00:45:14.086788: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-20 00:45:14.149342: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-12-20 00:45:14.149517: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-12-20 00:45:24.159777: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-12-20 00:45:24.160575: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-12-20 00:45:24.160705: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/xuandai/.local/lib/python3.7/site-packages/ipykernel_launcher.py:88: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "2023-12-20 00:45:39.332680: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-12-20 00:45:39.333333: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-12-20 00:45:39.333557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (My-PC): /proc/driver/nvidia/version does not exist\n",
      "2023-12-20 00:45:39.336333: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuandai/.local/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "216 fits failed out of a total of 432.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "216 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/xuandai/.local/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/wrappers/scikit_learn.py\", line 248, in fit\n",
      "    return super().fit(x, y, **kwargs)\n",
      "  File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/wrappers/scikit_learn.py\", line 175, in fit\n",
      "    history = self.model.fit(x, y, **fit_args)\n",
      "  File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/tmp/__autograph_generated_filem4bk78bd.py\", line 15, in tf__train_function\n",
      "    retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)\n",
      "ValueError: in user code:\n",
      "\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n",
      "        return step_function(self, iterator)\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n",
      "        outputs = model.train_step(data)\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 1024, in train_step\n",
      "        loss = self.compute_loss(x, y, y_pred, sample_weight)\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 1083, in compute_loss\n",
      "        y, y_pred, sample_weight, regularization_losses=self.losses\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n",
      "        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/losses.py\", line 152, in __call__\n",
      "        losses = call_fn(y_true, y_pred)\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/losses.py\", line 284, in call  **\n",
      "        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/losses.py\", line 2176, in binary_crossentropy\n",
      "        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/backend.py\", line 5681, in binary_crossentropy\n",
      "        labels=target, logits=output\n",
      "\n",
      "    ValueError: `logits` and `labels` must have the same shape, received ((None, 118) vs (None, 1)).\n",
      "\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/xuandai/.local/lib/python3.7/site-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.92337165 0.87547894 0.92720306 0.87547892 0.92337165 0.91954025\n",
      " 0.91954025 0.91954025 0.91570882 0.92337165 0.91570882 0.91954023\n",
      " 0.92528737 0.88888888 0.92720306 0.8927203  0.91762451 0.92337165\n",
      " 0.91954025 0.92145596 0.91570882 0.91954025 0.91570882 0.91954023\n",
      " 0.92720306 0.87356321 0.92720306 0.88697316 0.92145594 0.91954025\n",
      " 0.92337165 0.91954025 0.91570882 0.91954025 0.91570882 0.92528735\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.92720306 0.8927203  0.92720306 0.89463602 0.92145594 0.91954025\n",
      " 0.92145594 0.92145596 0.91570882 0.92337165 0.91762453 0.92720306\n",
      " 0.92720306 0.88505747 0.92528735 0.89846742 0.92337165 0.91954025\n",
      " 0.91762453 0.92145594 0.91570882 0.91954025 0.91570882 0.92528735\n",
      " 0.92720306 0.90038315 0.92720306 0.88505747 0.92720306 0.91954025\n",
      " 0.91954025 0.92145596 0.91570882 0.92720306 0.91570882 0.92145594]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'decay_rate': 0.9, 'decay_steps': 1000, 'dropout_cnn': 0.2, 'dropout_fc': 0.4, 'filtersize': 3, 'input_reshape': (277, 1), 'loss_func': 'sparse_categorical_crossentropy', 'lr_decay': 1e-06, 'lr_rate': 0.001, 'num_classes': 118, 'numfilter': 32, 'numlayercnn_per_maxpool': 1, 'optimizers_func': <class 'keras.optimizers.optimizer_experimental.adam.Adam'>, 'padded': True}\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1476/4190522657.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;31m# Train the model on the full training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;31m# Evaluate the best model on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    849\u001b[0m                     )\n\u001b[1;32m    850\u001b[0m                     for (cand_idx, parameters), (split_idx, (train, test)) in product(\n\u001b[0;32m--> 851\u001b[0;31m                         \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m                     )\n\u001b[1;32m    853\u001b[0m                 )\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1793\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid shape for y: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                         ):\n\u001b[1;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0;31m# no_variable_creation function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m       _, _, filtered_flat_args = (\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m    134\u001b[0m     return concrete_function._call_flat(\n\u001b[0;32m--> 135\u001b[0;31m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    381\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 53\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def create_cnn_model(input_reshape, num_classes, optimizers_func, lr_rate, lr_decay, decay_steps, decay_rate, loss_func, numfilter=16, filtersize=3, numlayercnn_per_maxpool=1, dropout_cnn=0.2, dropout_fc=0.5, padded=True):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv1D(filters=numfilter, kernel_size=filtersize, activation='relu', input_shape=input_reshape))\n",
    "    if padded:\n",
    "        model.add(tf.keras.layers.ZeroPadding1D(padding=(filtersize // 2)))\n",
    "    for _ in range(numlayercnn_per_maxpool):\n",
    "        model.add(tf.keras.layers.Conv1D(filters=numfilter, kernel_size=filtersize, activation='relu'))\n",
    "        if padded:\n",
    "            model.add(tf.keras.layers.ZeroPadding1D(padding=(filtersize // 2)))\n",
    "    model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(tf.keras.layers.Dropout(dropout_cnn))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(dropout_fc))\n",
    "    model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Using lr_rate for learning rate and lr_decay for learning rate decay\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=lr_rate,\n",
    "        decay_steps=decay_steps,  # Define the number of steps for decay\n",
    "        decay_rate=decay_rate,  # Define the decay rate\n",
    "        staircase=True)  # Optional: whether to decay the learning rate at discrete intervals\n",
    "\n",
    "    optimizer = optimizers_func(learning_rate=lr_schedule)\n",
    "    model.compile(optimizer=optimizer, loss=loss_func, metrics=['accuracy'])  # Using loss_func parameter here\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Load your data\n",
    "data = pd.read_csv('/mnt/d/20232024-01-CT252-B2003731_final/20232024-01-CT252-B2003731_final/deep-metagnomics-pub-main-final/data/Chatelier_gut_obesity_x.csv')\n",
    "data.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "data_transposed = data.transpose()\n",
    "label_column = data_transposed.columns[-1]\n",
    "X = data_transposed.drop(columns=[label_column])\n",
    "y = data_transposed[label_column]\n",
    "\n",
    "# Convert labels to strings\n",
    "y = y.astype(str)\n",
    "\n",
    "value_counts = y.value_counts()\n",
    "rare_classes = value_counts[value_counts < 2].index\n",
    "y = y[~y.isin(rare_classes)]\n",
    "X = X.loc[y.index]\n",
    "\n",
    "# Apply LabelEncoder to labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Define your decay_steps and decay_rate based on your dataset and requirements\n",
    "decay_steps = 1000  # Replace with your desired value\n",
    "decay_rate = 0.9  # Replace with your desired value\n",
    "\n",
    "# Reshape X for CNN\n",
    "X = X.values.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Further split the training set for parameter tuning\n",
    "X_train_part, _, y_train_part, _ = train_test_split(X_train, y_train, test_size=0.8, random_state=42)\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'input_reshape': [(X_train.shape[1], 1)],\n",
    "    'num_classes': [len(np.unique(y))],\n",
    "    'optimizers_func': [tf.keras.optimizers.Adam, tf.keras.optimizers.SGD],  # Pass optimizer classes directly\n",
    "    'lr_rate': [0.001, 0.01, 0.1],\n",
    "    'lr_decay': [1e-6, 1e-5, 1e-4],\n",
    "    'decay_steps': [decay_steps],\n",
    "    'decay_rate': [decay_rate],\n",
    "    'numfilter': [16, 32],\n",
    "    'filtersize': [3, 5],\n",
    "    'numlayercnn_per_maxpool': [1],\n",
    "    'dropout_cnn': [0.2],\n",
    "    'dropout_fc': [0.4],\n",
    "    'padded': [True],\n",
    "    'loss_func': ['binary_crossentropy', 'sparse_categorical_crossentropy'],  # Add both loss functions\n",
    "    # Include other hyperparameters you want to tune\n",
    "}\n",
    "# Create CNN model as a KerasClassifier\n",
    "cnn_model = tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn=create_cnn_model, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# Perform parameter tuning on the part of the training data\n",
    "grid_search = GridSearchCV(estimator=cnn_model, param_grid=param_grid, cv=3, verbose=1)\n",
    "grid_search.fit(X_train_part, y_train_part)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Train the model on the full training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "test_score = best_model.score(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8537881e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuandai/.local/lib/python3.7/site-packages/ipykernel_launcher.py:88: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuandai/.local/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "216 fits failed out of a total of 432.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "216 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/xuandai/.local/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/wrappers/scikit_learn.py\", line 248, in fit\n",
      "    return super().fit(x, y, **kwargs)\n",
      "  File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/wrappers/scikit_learn.py\", line 175, in fit\n",
      "    history = self.model.fit(x, y, **fit_args)\n",
      "  File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/tmp/__autograph_generated_filem4bk78bd.py\", line 15, in tf__train_function\n",
      "    retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)\n",
      "ValueError: in user code:\n",
      "\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n",
      "        return step_function(self, iterator)\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n",
      "        outputs = model.train_step(data)\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 1024, in train_step\n",
      "        loss = self.compute_loss(x, y, y_pred, sample_weight)\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 1083, in compute_loss\n",
      "        y, y_pred, sample_weight, regularization_losses=self.losses\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n",
      "        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/losses.py\", line 152, in __call__\n",
      "        losses = call_fn(y_true, y_pred)\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/losses.py\", line 284, in call  **\n",
      "        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/losses.py\", line 2176, in binary_crossentropy\n",
      "        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/backend.py\", line 5681, in binary_crossentropy\n",
      "        labels=target, logits=output\n",
      "\n",
      "    ValueError: `logits` and `labels` must have the same shape, received ((None, 131) vs (None, 1)).\n",
      "\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/xuandai/.local/lib/python3.7/site-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.90038315 0.85249043 0.90038313 0.88314176 0.89655171 0.90229885\n",
      " 0.90038313 0.90421456 0.89846744 0.90038313 0.89655173 0.90613025\n",
      " 0.90229885 0.86781609 0.90229885 0.86781609 0.89846742 0.90038313\n",
      " 0.90229887 0.90038313 0.89846744 0.90038313 0.89846744 0.90229885\n",
      " 0.90038313 0.86590038 0.90038313 0.87931035 0.90038313 0.89846744\n",
      " 0.90038313 0.90038313 0.89846744 0.90229885 0.89846744 0.90229885\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.90038313 0.86206897 0.90038313 0.8697318  0.90229885 0.90038313\n",
      " 0.90229885 0.90229885 0.89846744 0.90229885 0.89846744 0.90229885\n",
      " 0.90229885 0.86781609 0.90038313 0.8716475  0.89846742 0.89846744\n",
      " 0.89846742 0.90421454 0.89846744 0.90229885 0.89846744 0.90421456\n",
      " 0.90229885 0.86590038 0.90421456 0.87356321 0.90038315 0.90038313\n",
      " 0.89846744 0.90038313 0.89846744 0.90229885 0.89846744 0.90038313]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'decay_rate': 0.9, 'decay_steps': 1000, 'dropout_cnn': 0.2, 'dropout_fc': 0.4, 'filtersize': 3, 'input_reshape': (362, 1), 'loss_func': 'sparse_categorical_crossentropy', 'lr_decay': 1e-06, 'lr_rate': 0.1, 'num_classes': 131, 'numfilter': 32, 'numlayercnn_per_maxpool': 1, 'optimizers_func': <class 'keras.optimizers.optimizer_experimental.sgd.SGD'>, 'padded': True}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def create_cnn_model(input_reshape, num_classes, optimizers_func, lr_rate, lr_decay, decay_steps, decay_rate, loss_func, numfilter=16, filtersize=3, numlayercnn_per_maxpool=1, dropout_cnn=0.2, dropout_fc=0.5, padded=True):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv1D(filters=numfilter, kernel_size=filtersize, activation='relu', input_shape=input_reshape))\n",
    "    if padded:\n",
    "        model.add(tf.keras.layers.ZeroPadding1D(padding=(filtersize // 2)))\n",
    "    for _ in range(numlayercnn_per_maxpool):\n",
    "        model.add(tf.keras.layers.Conv1D(filters=numfilter, kernel_size=filtersize, activation='relu'))\n",
    "        if padded:\n",
    "            model.add(tf.keras.layers.ZeroPadding1D(padding=(filtersize // 2)))\n",
    "    model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(tf.keras.layers.Dropout(dropout_cnn))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(dropout_fc))\n",
    "    model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Using lr_rate for learning rate and lr_decay for learning rate decay\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=lr_rate,\n",
    "        decay_steps=decay_steps,  # Define the number of steps for decay\n",
    "        decay_rate=decay_rate,  # Define the decay rate\n",
    "        staircase=True)  # Optional: whether to decay the learning rate at discrete intervals\n",
    "\n",
    "    optimizer = optimizers_func(learning_rate=lr_schedule)\n",
    "    model.compile(optimizer=optimizer, loss=loss_func, metrics=['accuracy'])  # Using loss_func parameter here\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Load your data\n",
    "data = pd.read_csv('/mnt/d/20232024-01-CT252-B2003731_final/20232024-01-CT252-B2003731_final/deep-metagnomics-pub-main-final/data/t2d_x.csv')\n",
    "data.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "data_transposed = data.transpose()\n",
    "label_column = data_transposed.columns[-1]\n",
    "X = data_transposed.drop(columns=[label_column])\n",
    "y = data_transposed[label_column]\n",
    "\n",
    "# Convert labels to strings\n",
    "y = y.astype(str)\n",
    "\n",
    "value_counts = y.value_counts()\n",
    "rare_classes = value_counts[value_counts < 2].index\n",
    "y = y[~y.isin(rare_classes)]\n",
    "X = X.loc[y.index]\n",
    "\n",
    "# Apply LabelEncoder to labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Define your decay_steps and decay_rate based on your dataset and requirements\n",
    "decay_steps = 1000  # Replace with your desired value\n",
    "decay_rate = 0.9  # Replace with your desired value\n",
    "\n",
    "# Reshape X for CNN\n",
    "X = X.values.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Further split the training set for parameter tuning\n",
    "X_train_part, _, y_train_part, _ = train_test_split(X_train, y_train, test_size=0.8, random_state=42)\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'input_reshape': [(X_train.shape[1], 1)],\n",
    "    'num_classes': [len(np.unique(y))],\n",
    "    'optimizers_func': [tf.keras.optimizers.Adam, tf.keras.optimizers.SGD],  # Pass optimizer classes directly\n",
    "    'lr_rate': [0.001, 0.01, 0.1],\n",
    "    'lr_decay': [1e-6, 1e-5, 1e-4],\n",
    "    'decay_steps': [decay_steps],\n",
    "    'decay_rate': [decay_rate],\n",
    "    'numfilter': [16, 32],\n",
    "    'filtersize': [3, 5],\n",
    "    'numlayercnn_per_maxpool': [1],\n",
    "    'dropout_cnn': [0.2],\n",
    "    'dropout_fc': [0.4],\n",
    "    'padded': [True],\n",
    "    'loss_func': ['binary_crossentropy', 'sparse_categorical_crossentropy'],  # Add both loss functions\n",
    "    # Include other hyperparameters you want to tune\n",
    "}\n",
    "# Create CNN model as a KerasClassifier\n",
    "cnn_model = tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn=create_cnn_model, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# Perform parameter tuning on the part of the training data\n",
    "grid_search = GridSearchCV(estimator=cnn_model, param_grid=param_grid, cv=3, verbose=1)\n",
    "grid_search.fit(X_train_part, y_train_part)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbe08d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuandai/.local/lib/python3.7/site-packages/ipykernel_launcher.py:88: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuandai/.local/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "216 fits failed out of a total of 432.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "216 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/xuandai/.local/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/wrappers/scikit_learn.py\", line 248, in fit\n",
      "    return super().fit(x, y, **kwargs)\n",
      "  File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/wrappers/scikit_learn.py\", line 175, in fit\n",
      "    history = self.model.fit(x, y, **fit_args)\n",
      "  File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/tmp/__autograph_generated_filem4bk78bd.py\", line 15, in tf__train_function\n",
      "    retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)\n",
      "ValueError: in user code:\n",
      "\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n",
      "        return step_function(self, iterator)\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n",
      "        outputs = model.train_step(data)\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 1024, in train_step\n",
      "        loss = self.compute_loss(x, y, y_pred, sample_weight)\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 1083, in compute_loss\n",
      "        y, y_pred, sample_weight, regularization_losses=self.losses\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n",
      "        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/losses.py\", line 152, in __call__\n",
      "        losses = call_fn(y_true, y_pred)\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/losses.py\", line 284, in call  **\n",
      "        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/losses.py\", line 2176, in binary_crossentropy\n",
      "        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/backend.py\", line 5681, in binary_crossentropy\n",
      "        labels=target, logits=output\n",
      "\n",
      "    ValueError: `logits` and `labels` must have the same shape, received ((None, 88) vs (None, 1)).\n",
      "\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/xuandai/.local/lib/python3.7/site-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.94476191 0.91238095 0.94285715 0.91809525 0.94095238 0.93904761\n",
      " 0.93904761 0.93904761 0.93714285 0.94095238 0.93714285 0.93904763\n",
      " 0.94476191 0.89714285 0.94476191 0.92       0.94476191 0.93714285\n",
      " 0.9409524  0.93714285 0.93714285 0.94285715 0.93714285 0.93714285\n",
      " 0.94476191 0.91047621 0.94476191 0.92190476 0.93523808 0.93714285\n",
      " 0.94285715 0.93904761 0.93714285 0.94095238 0.93714285 0.93714285\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.94476191 0.91809523 0.94476191 0.91238095 0.94095238 0.93714285\n",
      " 0.94095238 0.93904761 0.93714285 0.94095238 0.93714285 0.94095238\n",
      " 0.94476191 0.90857144 0.94666668 0.91428572 0.94666668 0.93714285\n",
      " 0.93904761 0.93714285 0.93714285 0.94285715 0.93714285 0.93714285\n",
      " 0.94476191 0.89904761 0.94476191 0.91809523 0.93904761 0.93904761\n",
      " 0.94285715 0.94095238 0.93714285 0.94095238 0.93714285 0.9352381 ]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'decay_rate': 0.9, 'decay_steps': 1000, 'dropout_cnn': 0.2, 'dropout_fc': 0.4, 'filtersize': 5, 'input_reshape': (491, 1), 'loss_func': 'sparse_categorical_crossentropy', 'lr_decay': 1e-05, 'lr_rate': 0.001, 'num_classes': 88, 'numfilter': 32, 'numlayercnn_per_maxpool': 1, 'optimizers_func': <class 'keras.optimizers.optimizer_experimental.adam.Adam'>, 'padded': True}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def create_cnn_model(input_reshape, num_classes, optimizers_func, lr_rate, lr_decay, decay_steps, decay_rate, loss_func, numfilter=16, filtersize=3, numlayercnn_per_maxpool=1, dropout_cnn=0.2, dropout_fc=0.5, padded=True):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv1D(filters=numfilter, kernel_size=filtersize, activation='relu', input_shape=input_reshape))\n",
    "    if padded:\n",
    "        model.add(tf.keras.layers.ZeroPadding1D(padding=(filtersize // 2)))\n",
    "    for _ in range(numlayercnn_per_maxpool):\n",
    "        model.add(tf.keras.layers.Conv1D(filters=numfilter, kernel_size=filtersize, activation='relu'))\n",
    "        if padded:\n",
    "            model.add(tf.keras.layers.ZeroPadding1D(padding=(filtersize // 2)))\n",
    "    model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(tf.keras.layers.Dropout(dropout_cnn))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(dropout_fc))\n",
    "    model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Using lr_rate for learning rate and lr_decay for learning rate decay\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=lr_rate,\n",
    "        decay_steps=decay_steps,  # Define the number of steps for decay\n",
    "        decay_rate=decay_rate,  # Define the decay rate\n",
    "        staircase=True)  # Optional: whether to decay the learning rate at discrete intervals\n",
    "\n",
    "    optimizer = optimizers_func(learning_rate=lr_schedule)\n",
    "    model.compile(optimizer=optimizer, loss=loss_func, metrics=['accuracy'])  # Using loss_func parameter here\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Load your data\n",
    "data = pd.read_csv('/mnt/d/20232024-01-CT252-B2003731_final/20232024-01-CT252-B2003731_final/deep-metagnomics-pub-main-final/data/metahitIBD_x.csv')\n",
    "data.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "data_transposed = data.transpose()\n",
    "label_column = data_transposed.columns[-1]\n",
    "X = data_transposed.drop(columns=[label_column])\n",
    "y = data_transposed[label_column]\n",
    "\n",
    "# Convert labels to strings\n",
    "y = y.astype(str)\n",
    "\n",
    "value_counts = y.value_counts()\n",
    "rare_classes = value_counts[value_counts < 2].index\n",
    "y = y[~y.isin(rare_classes)]\n",
    "X = X.loc[y.index]\n",
    "\n",
    "# Apply LabelEncoder to labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Define your decay_steps and decay_rate based on your dataset and requirements\n",
    "decay_steps = 1000  # Replace with your desired value\n",
    "decay_rate = 0.9  # Replace with your desired value\n",
    "\n",
    "# Reshape X for CNN\n",
    "X = X.values.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Further split the training set for parameter tuning\n",
    "X_train_part, _, y_train_part, _ = train_test_split(X_train, y_train, test_size=0.8, random_state=42)\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'input_reshape': [(X_train.shape[1], 1)],\n",
    "    'num_classes': [len(np.unique(y))],\n",
    "    'optimizers_func': [tf.keras.optimizers.Adam, tf.keras.optimizers.SGD],  # Pass optimizer classes directly\n",
    "    'lr_rate': [0.001, 0.01, 0.1],\n",
    "    'lr_decay': [1e-6, 1e-5, 1e-4],\n",
    "    'decay_steps': [decay_steps],\n",
    "    'decay_rate': [decay_rate],\n",
    "    'numfilter': [16, 32],\n",
    "    'filtersize': [3, 5],\n",
    "    'numlayercnn_per_maxpool': [1],\n",
    "    'dropout_cnn': [0.2],\n",
    "    'dropout_fc': [0.4],\n",
    "    'padded': [True],\n",
    "    'loss_func': ['binary_crossentropy', 'sparse_categorical_crossentropy'],  # Add both loss functions\n",
    "    # Include other hyperparameters you want to tune\n",
    "}\n",
    "# Create CNN model as a KerasClassifier\n",
    "cnn_model = tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn=create_cnn_model, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# Perform parameter tuning on the part of the training data\n",
    "grid_search = GridSearchCV(estimator=cnn_model, param_grid=param_grid, cv=3, verbose=1)\n",
    "grid_search.fit(X_train_part, y_train_part)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d820a225",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuandai/.local/lib/python3.7/site-packages/ipykernel_launcher.py:88: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuandai/.local/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "216 fits failed out of a total of 432.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "216 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/xuandai/.local/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/wrappers/scikit_learn.py\", line 248, in fit\n",
      "    return super().fit(x, y, **kwargs)\n",
      "  File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/wrappers/scikit_learn.py\", line 175, in fit\n",
      "    history = self.model.fit(x, y, **fit_args)\n",
      "  File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/tmp/__autograph_generated_filem4bk78bd.py\", line 15, in tf__train_function\n",
      "    retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)\n",
      "ValueError: in user code:\n",
      "\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n",
      "        return step_function(self, iterator)\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n",
      "        outputs = model.train_step(data)\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 1024, in train_step\n",
      "        loss = self.compute_loss(x, y, y_pred, sample_weight)\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 1083, in compute_loss\n",
      "        y, y_pred, sample_weight, regularization_losses=self.losses\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n",
      "        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/losses.py\", line 152, in __call__\n",
      "        losses = call_fn(y_true, y_pred)\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/losses.py\", line 284, in call  **\n",
      "        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/losses.py\", line 2176, in binary_crossentropy\n",
      "        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/backend.py\", line 5681, in binary_crossentropy\n",
      "        labels=target, logits=output\n",
      "\n",
      "    ValueError: `logits` and `labels` must have the same shape, received ((None, 112) vs (None, 1)).\n",
      "\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/xuandai/.local/lib/python3.7/site-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.92145594 0.86781609 0.92337165 0.8716475  0.91762451 0.9157088\n",
      " 0.92337165 0.9157088  0.9157088  0.92145594 0.9157088  0.91954023\n",
      " 0.91954023 0.8697318  0.93103449 0.87931035 0.92145594 0.9157088\n",
      " 0.92337165 0.9157088  0.9157088  0.92145594 0.9157088  0.92145594\n",
      " 0.92337165 0.86206897 0.92720306 0.88314176 0.92720306 0.9157088\n",
      " 0.91954023 0.9157088  0.9157088  0.92528735 0.9157088  0.92337165\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.91954023 0.87739464 0.92145594 0.87547892 0.92145594 0.9157088\n",
      " 0.92145594 0.91762451 0.9157088  0.91954023 0.9157088  0.92145594\n",
      " 0.92337165 0.88122604 0.92145594 0.87356321 0.9157088  0.9157088\n",
      " 0.91954023 0.91762451 0.9157088  0.92337165 0.9157088  0.92337165\n",
      " 0.92145594 0.87931035 0.92145594 0.88697318 0.9157088  0.9157088\n",
      " 0.91762451 0.91762451 0.9157088  0.92720306 0.9157088  0.93103449]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'decay_rate': 0.9, 'decay_steps': 1000, 'dropout_cnn': 0.2, 'dropout_fc': 0.4, 'filtersize': 3, 'input_reshape': (133, 1), 'loss_func': 'sparse_categorical_crossentropy', 'lr_decay': 1e-05, 'lr_rate': 0.001, 'num_classes': 112, 'numfilter': 32, 'numlayercnn_per_maxpool': 1, 'optimizers_func': <class 'keras.optimizers.optimizer_experimental.adam.Adam'>, 'padded': True}\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuandai/.local/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "216 fits failed out of a total of 432.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "216 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/xuandai/.local/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/wrappers/scikit_learn.py\", line 248, in fit\n",
      "    return super().fit(x, y, **kwargs)\n",
      "  File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/wrappers/scikit_learn.py\", line 175, in fit\n",
      "    history = self.model.fit(x, y, **fit_args)\n",
      "  File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/tmp/__autograph_generated_filem4bk78bd.py\", line 15, in tf__train_function\n",
      "    retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)\n",
      "ValueError: in user code:\n",
      "\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n",
      "        return step_function(self, iterator)\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n",
      "        outputs = model.train_step(data)\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 1024, in train_step\n",
      "        loss = self.compute_loss(x, y, y_pred, sample_weight)\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 1083, in compute_loss\n",
      "        y, y_pred, sample_weight, regularization_losses=self.losses\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n",
      "        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/losses.py\", line 152, in __call__\n",
      "        losses = call_fn(y_true, y_pred)\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/losses.py\", line 284, in call  **\n",
      "        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/losses.py\", line 2176, in binary_crossentropy\n",
      "        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n",
      "    File \"/home/xuandai/.local/lib/python3.7/site-packages/keras/backend.py\", line 5681, in binary_crossentropy\n",
      "        labels=target, logits=output\n",
      "\n",
      "    ValueError: `logits` and `labels` must have the same shape, received ((None, 112) vs (None, 1)).\n",
      "\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/xuandai/.local/lib/python3.7/site-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.93223184 0.91577129 0.93299901 0.91500588 0.92687358 0.91883554\n",
      " 0.91730473 0.91998275 0.91500588 0.92610774 0.91500588 0.92763855\n",
      " 0.93146733 0.91500588 0.93108374 0.91538902 0.92265991 0.91960094\n",
      " 0.93491165 0.91959961 0.91500588 0.92610774 0.91500588 0.92572417\n",
      " 0.93146908 0.91500588 0.93108507 0.91538902 0.92610466 0.91960005\n",
      " 0.91806924 0.92036766 0.91500588 0.9241938  0.91500588 0.92916936\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.93261631 0.91500588 0.9310833  0.91500588 0.92687535 0.91806881\n",
      " 0.92687358 0.9188351  0.91500588 0.9241938  0.91500588 0.92917069\n",
      " 0.93261544 0.91500588 0.93338172 0.91500588 0.92304436 0.91806881\n",
      " 0.9180675  0.91692026 0.91500588 0.92610731 0.91500588 0.92572373\n",
      " 0.93223361 0.91500588 0.9310842  0.91500588 0.91845415 0.91883465\n",
      " 0.919981   0.91730297 0.9161553  0.9253419  0.91500588 0.92878666]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9204893112182617\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def create_cnn_model(input_reshape, num_classes, optimizers_func, lr_rate, lr_decay, decay_steps, decay_rate, loss_func, numfilter=16, filtersize=3, numlayercnn_per_maxpool=1, dropout_cnn=0.2, dropout_fc=0.5, padded=True):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv1D(filters=numfilter, kernel_size=filtersize, activation='relu', input_shape=input_reshape))\n",
    "    if padded:\n",
    "        model.add(tf.keras.layers.ZeroPadding1D(padding=(filtersize // 2)))\n",
    "    for _ in range(numlayercnn_per_maxpool):\n",
    "        model.add(tf.keras.layers.Conv1D(filters=numfilter, kernel_size=filtersize, activation='relu'))\n",
    "        if padded:\n",
    "            model.add(tf.keras.layers.ZeroPadding1D(padding=(filtersize // 2)))\n",
    "    model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(tf.keras.layers.Dropout(dropout_cnn))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(dropout_fc))\n",
    "    model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Using lr_rate for learning rate and lr_decay for learning rate decay\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=lr_rate,\n",
    "        decay_steps=decay_steps,  # Define the number of steps for decay\n",
    "        decay_rate=decay_rate,  # Define the decay rate\n",
    "        staircase=True)  # Optional: whether to decay the learning rate at discrete intervals\n",
    "\n",
    "    optimizer = optimizers_func(learning_rate=lr_schedule)\n",
    "    model.compile(optimizer=optimizer, loss=loss_func, metrics=['accuracy'])  # Using loss_func parameter here\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Load your data\n",
    "data = pd.read_csv('/mnt/d/20232024-01-CT252-B2003731_final/20232024-01-CT252-B2003731_final/deep-metagnomics-pub-main-final/data/Zeller_fecal_colorectal_cancer_x.csv')\n",
    "data.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "data_transposed = data.transpose()\n",
    "label_column = data_transposed.columns[-1]\n",
    "X = data_transposed.drop(columns=[label_column])\n",
    "y = data_transposed[label_column]\n",
    "\n",
    "# Convert labels to strings\n",
    "y = y.astype(str)\n",
    "\n",
    "value_counts = y.value_counts()\n",
    "rare_classes = value_counts[value_counts < 2].index\n",
    "y = y[~y.isin(rare_classes)]\n",
    "X = X.loc[y.index]\n",
    "\n",
    "# Apply LabelEncoder to labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Define your decay_steps and decay_rate based on your dataset and requirements\n",
    "decay_steps = 1000  # Replace with your desired value\n",
    "decay_rate = 0.9  # Replace with your desired value\n",
    "\n",
    "# Reshape X for CNN\n",
    "X = X.values.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Further split the training set for parameter tuning\n",
    "X_train_part, _, y_train_part, _ = train_test_split(X_train, y_train, test_size=0.8, random_state=42)\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'input_reshape': [(X_train.shape[1], 1)],\n",
    "    'num_classes': [len(np.unique(y))],\n",
    "    'optimizers_func': [tf.keras.optimizers.Adam, tf.keras.optimizers.SGD],  # Pass optimizer classes directly\n",
    "    'lr_rate': [0.001, 0.01, 0.1],\n",
    "    'lr_decay': [1e-6, 1e-5, 1e-4],\n",
    "    'decay_steps': [decay_steps],\n",
    "    'decay_rate': [decay_rate],\n",
    "    'numfilter': [16, 32],\n",
    "    'filtersize': [3, 5],\n",
    "    'numlayercnn_per_maxpool': [1],\n",
    "    'dropout_cnn': [0.2],\n",
    "    'dropout_fc': [0.4],\n",
    "    'padded': [True],\n",
    "    'loss_func': ['binary_crossentropy', 'sparse_categorical_crossentropy'],  # Add both loss functions\n",
    "    # Include other hyperparameters you want to tune\n",
    "}\n",
    "# Create CNN model as a KerasClassifier\n",
    "cnn_model = tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn=create_cnn_model, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# Perform parameter tuning on the part of the training data\n",
    "grid_search = GridSearchCV(estimator=cnn_model, param_grid=param_grid, cv=3, verbose=1)\n",
    "grid_search.fit(X_train_part, y_train_part)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Train the model on the full training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "test_score = best_model.score(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f10aac4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
